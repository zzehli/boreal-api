Score Percentages:
Helpfulness: 92.3%
Groundedness: 53.8%
Retrieval: 76.9%

This evaluation leverage the [RAG triad](https://truera.com/ai-quality-education/generative-ai-rags/what-is-the-rag-triad/) metrics proposed by Snowflake's TruEra. The triad consists of answer relevance, context relevance and Groundedness. In my case, the three corresponding metrics are helpfulness, which measures how well the generated response addresses the initial user input, groundedness, which measures the extent that the generated response agrees with the retrieved context, and retrieval relevance, which measures how relevant retrieved context is to an input query. The metrics prompts I use is developed by Langchain's `openeval`.

As the score shows, the helpfulness score is quite high, which indicates that the final answer generated by the rag system generally produces useful information for the user. The retrieval score means the materials retrieved are generally helpful. Future improvement to improve the retrieval include tuning data chunking methods, as well as using different retrieval algorithm. The latter has many moving parts. I am using hybrid search for my RAG retrieval, which involve a full-text search (bm-25), a vector search (HNSW), and a score merging process (RRF). All three can be adjusted.

The groundedness received lowest score, which means the final response isn't based on the retrieved documents. This metrics is a measure of a model's hallucination. However, the caveat is that in my case, after the first retrieval based on embedding, I performed another round of retrieval based on meta data (urls) to fetch full page content of retrieved segments. This ensures the llm can see the entirety of a page, which evidently produces helpful results.



