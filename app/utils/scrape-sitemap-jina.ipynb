{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_english_links(xml_file):\n",
    "    # Define the XML namespaces\n",
    "    namespaces = {\n",
    "        'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9',\n",
    "        'xhtml': 'http://www.w3.org/1999/xhtml'\n",
    "    }\n",
    "    \n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Create a list to store English URLs\n",
    "    english_urls = []\n",
    "    \n",
    "    # Find all URL elements\n",
    "    for url in root.findall('.//ns:url', namespaces):\n",
    "        # Find all xhtml:link elements within each URL\n",
    "        for link in url.findall('.//xhtml:link', namespaces):\n",
    "            # Check if the link is English\n",
    "            if link.get('hreflang') == 'en':\n",
    "                english_urls.append(link.get('href'))\n",
    "    \n",
    "    return english_urls\n",
    "\n",
    "def save_urls_to_file(urls, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for url in urls:\n",
    "            f.write(f\"{url}\\n\")\n",
    "\n",
    "xml_file = '../data/sitemap.xml'  # Replace with your sitemap file path\n",
    "output_file = '../data/english_urls.txt'\n",
    "\n",
    "# Extract and save URLs\n",
    "english_urls = extract_english_links(xml_file)\n",
    "save_urls_to_file(english_urls, output_file)\n",
    "\n",
    "print(f\"Found {len(english_urls)} English URLs and saved them to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "target_url = \"https://www.haagen-dazs.ca/en/haagen-dazs/extraaz-cookie-dough-dynamo-ice-cream-single-bars\"\n",
    "url = 'https://r.jina.ai/' + target_url\n",
    "headers = {\n",
    "    'Authorization': 'Bearer',\n",
    "    'X-Remove-Selector': '.latest-article-slider, #block-views-block-recent-videos-recent-videos, .product-recipes-container',\n",
    "    'X-Retain-Images': 'none',\n",
    "    \"X-Base\": \"final\",\n",
    "    'X-Target-Selector': 'main',\n",
    "    'X-With-Links-Summary': 'true'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import List, Set\n",
    "from dotenv import load_dotenv\n",
    "import csv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "# for jupyter only\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/scraping.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, urls_file: str, log_dir: str, output_dir: str, max_concurrent: int):\n",
    "        self.urls_file = urls_file\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self.successful_urls: Set[str] = set()\n",
    "        self.failed_urls: Set[str] = set()\n",
    "        self.processed_urls: List[tuple] = []  # List of (filename, url) tuples\n",
    "        self.headers = {\n",
    "            'X-Remove-Selector': '.latest-article-slider, #block-views-block-recent-videos-recent-videos, .product-recipes-container',\n",
    "            'X-Retain-Images': 'none',\n",
    "            'X-Base': 'final',\n",
    "            'X-Target-Selector': 'main',\n",
    "            'X-With-Links-Summary': 'true',\n",
    "        }\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create a queue for URLs\n",
    "        self.queue = asyncio.Queue()\n",
    "        \n",
    "    async def load_urls(self):\n",
    "        \"\"\"Load URLs from file into queue, skipping those with existing files\"\"\"\n",
    "        # Get list of existing files\n",
    "        existing_files = set(f.stem for f in self.output_dir.glob('*.md'))\n",
    "        \n",
    "        with open(self.urls_file, 'r') as f:\n",
    "            urls = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        skipped = 0\n",
    "        queued = 0\n",
    "        \n",
    "        for url in urls:\n",
    "            # Generate the expected filename using same logic as in process_url\n",
    "            path = url.split('://', 1)[-1].split('/', 1)[-1]\n",
    "            if path.startswith(('video/', 'node/')):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            parts = [p for p in path.split('/') if p]\n",
    "            expected_filename = self.sanitize_filename(\"_\".join(parts))\n",
    "            \n",
    "            # Check if file already exists\n",
    "            if expected_filename in existing_files:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Queue URL if file doesn't exist\n",
    "            await self.queue.put(url)\n",
    "            queued += 1\n",
    "        \n",
    "        logging.info(f\"Loaded {queued} URLs into queue, skipped {skipped} existing files\")\n",
    "    \n",
    "    def sanitize_filename(self, filename: str) -> str:\n",
    "        # Remove invalid characters\n",
    "        invalid_chars = '<>:\"/\\\\|?*'\n",
    "        for char in invalid_chars:\n",
    "            filename = filename.replace(char, '_')\n",
    "        return filename\n",
    "\n",
    "    async def process_url(self, session: aiohttp.ClientSession, url: str, retries=2):\n",
    "        \"\"\"Process a single URL\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                api_url = f'https://r.jina.ai/{url}'\n",
    "                async with session.get(api_url, headers=self.headers) as response:\n",
    "                    if response.status == 200:\n",
    "                        content = await response.text()\n",
    "                        \n",
    "                        path = url.split('://', 1)[-1].split('/', 1)[-1]\n",
    "\n",
    "                        # Split by '/' and filter out empty parts\n",
    "                        parts = [p for p in path.split('/') if p]\n",
    "\n",
    "                        # Join with underscore\n",
    "                        filename = self.sanitize_filename(\"_\".join(parts))\n",
    "                        # If empty or just a slash, use 'index'\n",
    "                        if not filename:\n",
    "                            filename = 'index'\n",
    "                        filepath = self.output_dir / f\"{filename}.md\"\n",
    "                        \n",
    "                        # Save content\n",
    "                        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                            f.write(content)\n",
    "                        \n",
    "                        self.processed_urls.append((f\"{filename}.md\", url))\n",
    "\n",
    "                        \n",
    "                        self.successful_urls.add(url)\n",
    "                        logging.info(f\"Successfully processed {url}\")\n",
    "                    else:\n",
    "                        self.failed_urls.add(url)\n",
    "                        logging.error(f\"Failed to process {url}: Status {response.status}\")\n",
    "            except Exception as e:\n",
    "                if attempt < retries - 1:\n",
    "                    logging.warning(f\"Attempt {attempt+1}/{retries} failed for {url}. Retrying in 20s\")\n",
    "                    await asyncio.sleep(20)\n",
    "                else:\n",
    "                    self.failed_urls.add(url)\n",
    "                    logging.error(f\"Error processing {url}: {str(e)}\")\n",
    "                    logging.error(f\"Error type: {type(e).__name__}\")\n",
    "                    if hasattr(e, 'response'):\n",
    "                        logging.error(f\"Response status: {e.response.status if hasattr(e.response, 'status') else 'N/A'}\")\n",
    "\n",
    "    async def worker(self, session: aiohttp.ClientSession):\n",
    "        \"\"\"Worker to process URLs from queue\"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                url = await self.queue.get()\n",
    "                await self.process_url(session, url)\n",
    "                self.queue.task_done()\n",
    "            except asyncio.CancelledError:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Worker error: {str(e)}\")\n",
    "                self.failed_urls.add(url)\n",
    "                self.queue.task_done()\n",
    "\n",
    "    async def run(self):\n",
    "        \"\"\"Run the scraper\"\"\"\n",
    "        timeout = aiohttp.ClientTimeout(total=20)\n",
    "        async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "            workers = [\n",
    "                asyncio.create_task(self.worker(session))\n",
    "                for _ in range(self.max_concurrent)\n",
    "            ]\n",
    "            \n",
    "            # Wait for all URLs to be processed\n",
    "            await self.queue.join()\n",
    "            for w in workers:\n",
    "                w.cancel()\n",
    "            \n",
    "            # Wait for workers to finish\n",
    "            await asyncio.gather(*workers, return_exceptions=True)\n",
    "    \n",
    "    def save_processed_urls(self):\n",
    "        \"\"\"Save processed URLs to CSV file\"\"\"\n",
    "        with open(self.log_dir / 'processed_urls.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['filename', 'url'])\n",
    "            writer.writerows(self.processed_urls)\n",
    "            logging.info(f\"Saved {len(self.processed_urls)} processed URLs\")\n",
    "        \n",
    "        with open(self.log_dir / 'successful_urls.txt', 'w') as f:\n",
    "            for url in self.successful_urls:\n",
    "                f.write(f\"{url}\\n\")\n",
    "        \n",
    "        # Save failed URLs\n",
    "        with open(self.log_dir / 'failed_urls.txt', 'w') as f:\n",
    "            for url in self.failed_urls:\n",
    "                f.write(f\"{url}\\n\")\n",
    "        \n",
    "        logging.info(f\"Processed {len(self.successful_urls)} URLs successfully\")\n",
    "        logging.info(f\"Failed to process {len(self.failed_urls)} URLs\")\n",
    "\n",
    "async def main():\n",
    "    scraper = Scraper(\n",
    "        urls_file='../data/english_urls.txt',\n",
    "        output_dir='../data/site',\n",
    "        log_dir='./logs',\n",
    "        max_concurrent=5\n",
    "    )\n",
    "    \n",
    "    await scraper.load_urls()\n",
    "    await scraper.run()\n",
    "    scraper.save_processed_urls()\n",
    "\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
