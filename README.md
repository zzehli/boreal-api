# Boreal RAG API
## Features
- Implements an Agentic Retrieval-Augmented Generation system with Langgraph. The system decides between conversation (solicit clarifications) and information retrieval 
- Use Azure AI search for hybrid retrieval; it combines BM25 (full-text search) and HNSW (vector search) with Reciprocal Rank Fusion (RRF) for optimal retrieval results 
- Comprehensive evaluation using the RAG triad metrics (helpfulness, groundedness, retrieval relevance) with `openevals`
- Deployed dedicated OpenAI o4 model and `text-embedding-3-large` embedding model with Azure AI foundry
- Uses crawl4ai for memory aware, concurrent, llm optimized scraping
- Production grade versioned APIs with FastAPI
- Deployed on a Azure VM with Caddy reverse proxy

For fronend features, check: https://github.com/zzehli/boreal-chat
## Local Development
The project use uv to manage packages. Pull the codebase and install the packages with
```
uv sync
```
Fill in the dot env file.
Set the PYTHONPATH
```
export PYTHONPATH=$PWD
```
Run the service:
```
fastapi dev app/main.py
```

## Future Development
- Implement and test different text chunking strategies and experiment with different chunking lengths
- Experiment with different retrieval algorithm (different hybrid research parameters, incorporate reranking, etc)
- Currently, the I implement an extra step to retrieve full web page based on retrieved segments, but the retrieved content are not pieced together. To optimize the retrieval process, incorporate langchain's `ParentDocument` retriever to automatically fetch the whole document. But the tradeoff is to use extra storage space to store whole documents
- Integrate evaluation with graph proper. RN, the eval implements a separate pipeline to fit the `openeval` evaluators. More work can be done to integrate these eval with the api itself, so that the eval can be performed on operation data
- Implement cron jobs to crawl source websites on schedule
- Implement CICD pipeline to run eval and deployment on github push

## Eval
Score Percentages:
Helpfulness: 92.3%
Groundedness: 100%
Retrieval: 76.9%

This evaluation leverage the [RAG triad](https://truera.com/ai-quality-education/generative-ai-rags/what-is-the-rag-triad/) metrics proposed by Snowflake's TruEra. The triad consists of answer relevance, context relevance and Groundedness. In my case, the three corresponding metrics are helpfulness, which measures how well the generated response addresses the initial user input, groundedness, which measures the extent that the generated response agrees with the retrieved context, and retrieval relevance, which measures how relevant retrieved context is to an input query. The metrics prompts I use is developed by Langchain's `openeval`.

As the score shows, the helpfulness score is quite high, which indicates that the final answer generated by the rag system generally produces useful information for the user. The retrieval score means the materials retrieved are generally helpful. Future improvement to improve the retrieval include tuning data chunking methods, as well as using different retrieval algorithm. The latter has many moving parts. I am using hybrid search for my RAG retrieval, which involve a full-text search, a vector search, and a score merging process. All three can be adjusted. The groundedness is mostly a metric about llm model than a llm system metric. It is a measure of a model's hallucination. As long as the system doesn't hallucinate, the groundedness metric will be high.
